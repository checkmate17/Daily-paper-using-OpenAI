{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from arxplore.datamodel import Feed, Config\n",
    "\n",
    "\n",
    "def parse_feed(namespace: str, config: Config) -> List[Feed]:\n",
    "    url = f\"https://arxiv.org/list/{namespace}/new\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    feeds = []\n",
    "    for entry in soup.find_all(\"dl\"):\n",
    "        title = entry.find(\"div\", {\"class\": \"list-title\"}).text\n",
    "        abstract = entry.find(\"div\", {\"class\": \"abstract\"}).text\n",
    "        authors = entry.find(\"div\", {\"class\": \"list-authors\"}).text\n",
    "        url = entry.find(\"a\", {\"title\": \"Abstract\"}).get(\"href\")\n",
    "        feed = Feed(title=title, abstract=abstract, authors=authors, url=url)\n",
    "        feeds.append(feed)\n",
    "    return feeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"cs.AI\"\n",
    "url = f\"https://arxiv.org/list/{namespace}/new\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the page\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully retrieved the page\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page, status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_blocks = soup.find_all('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dt><a name=\"item1\">[1]</a>Â   <span class=\"list-identifier\"><a href=\"/abs/2402.09413\" title=\"Abstract\">arXiv:2402.09413</a> [<a href=\"/pdf/2402.09413\" title=\"Download PDF\">pdf</a>, <a href=\"/ps/2402.09413\" title=\"Download PostScript\">ps</a>, <a href=\"/format/2402.09413\" title=\"Other formats\">other</a>]</span></dt>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = paper_blocks[0]\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mathematical Explanations\n",
      "Authors: Joseph Y. Halpern\n",
      "Abstract: A definition of what counts as an explanation of mathematical statement, and\n",
      "when one explanation is better than another, is given. Since all mathematical\n",
      "facts must be true in all causal models, and hence known by an agent,\n",
      "mathematical facts cannot be part of an explanation (under the standard notion\n",
      "of explanation). This problem is solved using impossible possible worlds.\n",
      "PDF URL: https://arxiv.org/pdf/2402.09413\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for block in paper_blocks:\n",
    "    # Find the <dd> tag that immediately follows each <dt> tag\n",
    "    metadata = block.find_next_sibling('dd')\n",
    "    title = metadata.find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "    authors = [a.text for a in metadata.find('div', class_='list-authors').find_all('a')]\n",
    "    abstract = metadata.find('p').text.strip()\n",
    "    # Extract the PDF link from the <dt> block\n",
    "    pdf_link_suffix = block.find('a', title='Download PDF')['href']\n",
    "    pdf_url = f'https://arxiv.org{pdf_link_suffix}'\n",
    "    print(f'Title: {title}\\nAuthors: {\", \".join(authors)}\\nAbstract: {abstract}\\nPDF URL: {pdf_url}\\n{\"-\"*40}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m abstract \u001b[38;5;241m=\u001b[39m paper\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Navigate to the previous sibling to find the PDF link\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m list_identifier \u001b[38;5;241m=\u001b[39m \u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_previous_sibling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist-identifier\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m pdf_link_suffix \u001b[38;5;241m=\u001b[39m list_identifier\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownload PDF\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m pdf_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_link_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    title = paper.find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "    authors = [a.text for a in paper.find('div', class_='list-authors').find_all('a')]\n",
    "    abstract = paper.find('p').text.strip()\n",
    "    # Navigate to the previous sibling to find the PDF link\n",
    "    list_identifier = paper.find_previous_sibling('dt').find('span', class_='list-identifier')\n",
    "    pdf_link_suffix = list_identifier.find('a', title='Download PDF')['href']\n",
    "    pdf_url = f'https://arxiv.org{pdf_link_suffix}'\n",
    "    print(f'Title: {title}\\nAuthors: {\", \".join(authors)}\\nAbstract: {abstract}\\nURL: {url}\\n{\"-\"*40}')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Binfeng Xu\n",
      "Affiliation: New York University\n",
      "Interests: ['Machine Learning']\n",
      "Cited by: 36\n",
      "h-index: 2\n",
      "i10-index: 2\n",
      "Number of publications: 3\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Replace 'Joseph Y. Halpern' with the name of the author you are interested in\n",
    "author_name = 'Binfeng Xu'\n",
    "\n",
    "try:\n",
    "    # Search for the author and take the first result\n",
    "    search_query = scholarly.search_author(author_name)\n",
    "    author = next(search_query)\n",
    "    \n",
    "    # Fill in more detailed information about the author\n",
    "    scholarly.fill(author, sections=['basics', 'indices', 'counts', 'publications'])\n",
    "    \n",
    "    print(f\"Name: {author['name']}\")\n",
    "    print(f\"Affiliation: {author.get('affiliation')}\")\n",
    "    print(f\"Interests: {author.get('interests', [])}\")\n",
    "    print(f\"Cited by: {author['citedby']}\")\n",
    "    print(f\"h-index: {author['hindex']}\")\n",
    "    print(f\"i10-index: {author['i10index']}\")\n",
    "    print(f\"Number of publications: {len(author['publications'])}\")\n",
    "except StopIteration:\n",
    "    print(\"Author not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author.get(\"citedby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxplore.parsers import parse_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 156 papers in the cs.AI section of arXiv.\n",
      "\n",
      "Parsing information for Sujay Nagesh Koujalgi from Google Scholar...\n",
      "Parsing information for Jonathan Dodge from Google Scholar...\n",
      "Parsing information for Lance Ying from Google Scholar...\n",
      "Parsing information for Tan Zhi-Xuan from Google Scholar...\n",
      "Parsing information for Lionel Wong from Google Scholar...\n",
      "Parsing information for Vikash Mansinghka from Google Scholar...\n",
      "Parsing information for Joshua Tenenbaum from Google Scholar...\n",
      "Parsing information for Yiwen Sun from Google Scholar...\n",
      "Parsing information for Xianyin Zhang from Google Scholar...\n",
      "Parsing information for Shiyu Huang from Google Scholar...\n",
      "Parsing information for Shaowei Cai from Google Scholar...\n",
      "Parsing information for Bing-Zhen Zhang from Google Scholar...\n",
      "Parsing information for Ke Wei from Google Scholar...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10290', title='Experiments with Encoding Structured Data for Neural Networks', authors=[Author(name='Sujay Nagesh Koujalgi', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Jonathan Dodge', affiliation='Assistant Professor, Penn State University', interests=['Explainable AI', 'Human-Computer Interaction', 'Graphics'], citation=1120, h_index=12, n_publications=33)], f_author=Author(name='Sujay Nagesh Koujalgi', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), abstract=\"The project's aim is to create an AI agent capable of selecting good actions\\nin a game-playing domain called Battlespace. Sequential domains like\\nBattlespace are important testbeds for planning problems, as such, the\\nDepartment of Defense uses such domains for wargaming exercises. The agents we\\ndeveloped combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN)\\ntechniques in an effort to navigate the game environment, avoid obstacles,\\ninteract with adversaries, and capture the flag. This paper will focus on the\\nencoding techniques we explored to present complex structured data stored in a\\nPython class, a necessary precursor to an agent.\"),\n",
       " Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10416', title='Grounding Language about Belief in a Bayesian Theory-of-Mind', authors=[Author(name='Lance Ying', affiliation='Harvard University', interests=['Bayesian Modeling', 'Theory of Mind', 'Human-AI Interaction'], citation=8, h_index=2, n_publications=5), Author(name='Tan Zhi-Xuan', affiliation='MIT', interests=['Probabilistic Programming', 'Artificial intelligence', 'Computational Cognitive Science', 'Value Alignment'], citation=310, h_index=8, n_publications=19), Author(name='Lionel Catherine Wong', affiliation='Massachusetts Institute of Technology', interests=[], citation=799, h_index=12, n_publications=31), Author(name='Vikash K. Mansinghka', affiliation='MIT, Probabilistic Computing Project', interests=['artificial intelligence', 'statistics', 'probabilistic programming', 'machine learning'], citation=4551, h_index=28, n_publications=156), Author(name='Joshua B. Tenenbaum', affiliation='MIT', interests=['Cognitive science', 'artificial intelligence', 'machine learning', 'computational neuroscience', 'cognitive psychology'], citation=104081, h_index=138, n_publications=1016)], f_author=Author(name='Lance Ying', affiliation='Harvard University', interests=['Bayesian Modeling', 'Theory of Mind', 'Human-AI Interaction'], citation=8, h_index=2, n_publications=5), abstract=\"Despite the fact that beliefs are mental states that cannot be directly\\nobserved, humans talk about each others' beliefs on a regular basis, often\\nusing rich compositional language to describe what others think and know. What\\nexplains this capacity to interpret the hidden epistemic content of other\\nminds? In this paper, we take a step towards an answer by grounding the\\nsemantics of belief statements in a Bayesian theory-of-mind: By modeling how\\nhumans jointly infer coherent sets of goals, beliefs, and plans that explain an\\nagent's actions, then evaluating statements about the agent's beliefs against\\nthese inferences via epistemic logic, our framework provides a conceptual role\\nsemantics for belief, explaining the gradedness and compositionality of human\\nbelief attributions, as well as their intimate connection with goals and plans.\\nWe evaluate this framework by studying how humans attribute goals and beliefs\\nwhile watching an agent solve a doors-and-keys gridworld puzzle that requires\\ninstrumental reasoning about hidden objects. In contrast to pure logical\\ndeduction, non-mentalizing baselines, and mentalizing that ignores the role of\\ninstrumental plans, our model provides a much better fit to human goal and\\nbelief attributions, demonstrating the importance of theory-of-mind for a\\nsemantics of belief.\"),\n",
       " Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10705', title='AutoSAT: Automatically Optimize SAT Solvers via Large Language Models', authors=[Author(name='Yiwen Zhang', affiliation='Sun Yat-sen University', interests=['Immunology', 'Virology'], citation=1605, h_index=16, n_publications=32), Author(name='Xianyin Zhang', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Shi-Yu Huang', affiliation='Professor of Eletrical Engineering, National Tsing Hua University', interests=['VLSI Testing', 'IC Design', 'Design Automation'], citation=3404, h_index=31, n_publications=358), Author(name='Shaowei Cai', affiliation='Institute of Software, Chinese Academy of Sciences', interests=['Satisfiability', 'Constraint Solving', 'Combinatorial Optimization', 'Heuristic Search'], citation=3040, h_index=33, n_publications=136), Author(name='Bing-Zhen Zhang', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Kewei Chen', affiliation=\"Banner Alzheimer's Institute, Phoenix, AZ\", interests=[\"Alzheimer's\", 'neuroimage', 'MRI', 'PET', 'statistics'], citation=41357, h_index=106, n_publications=851)], f_author=Author(name='Yiwen Zhang', affiliation='Sun Yat-sen University', interests=['Immunology', 'Virology'], citation=1605, h_index=16, n_publications=32), abstract='Heuristics are crucial in SAT solvers, while no heuristic rules are suitable\\nfor all problem instances. Therefore, it typically requires to refine specific\\nsolvers for specific problem instances. In this context, we present AutoSAT, a\\nnovel framework for automatically optimizing heuristics in SAT solvers. AutoSAT\\nis based on Large Large Models (LLMs) which is able to autonomously generate\\ncode, conduct evaluation, then utilize the feedback to further optimize\\nheuristics, thereby reducing human intervention and enhancing solver\\ncapabilities. AutoSAT operates on a plug-and-play basis, eliminating the need\\nfor extensive preliminary setup and model training, and fosters a Chain of\\nThought collaborative process with fault-tolerance, ensuring robust heuristic\\noptimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL)\\nsolver demonstrates the overall superior performance of AutoSAT, especially in\\nsolving some specific SAT problem instances.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_arxiv(\"cs.AI\", tests = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists!\n"
     ]
    }
   ],
   "source": [
    "from arxplorer.db import init_db\n",
    "from arxplorer.parsers import _parse_scholar\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author(name='Binfeng Xu', affiliation='New York University', interests='Machine Learning', citation=36, h_index=2, n_publications=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_scholar(\"Binfeng Xu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author(name='Binfeng Xu', affiliation='New York University', interests='Machine Learning', citation=36, h_index=2, n_publications=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_scholar(\"Binfeng Xu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'authors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([(a\u001b[38;5;241m.\u001b[39mh_index \u001b[38;5;241m-\u001b[39m avg) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors)\n\u001b[1;32m     37\u001b[0m fe \u001b[38;5;241m=\u001b[39m FeatureExtractor([])\n\u001b[0;32m---> 38\u001b[0m \u001b[43mfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_authors_citation\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mFeatureExtractor.avg_authors_citation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_authors_citation\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([a\u001b[38;5;241m.\u001b[39mcitation \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthors\u001b[49m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'authors'"
     ]
    }
   ],
   "source": [
    "# Defining useful signals for ranking\n",
    "\n",
    "from arxplorer.datamodel import Feed\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, feeds: list[Feed]):\n",
    "        self.feed = feeds\n",
    "\n",
    "    @property\n",
    "    def first_author_citation(self) -> int:\n",
    "        return self.feed.authors[0].citation\n",
    "\n",
    "    @property\n",
    "    def avg_authors_citation(self) -> float:\n",
    "        return sum([a.citation for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def variance_authors_citation(self) -> float:\n",
    "        avg = self.avg_authors_citation\n",
    "        return sum([(a.citation - avg) ** 2 for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def first_author_h_index(self) -> int:\n",
    "        return self.feed.authors[0].h_index\n",
    "\n",
    "    @property\n",
    "    def avg_authors_h_index(self) -> float:\n",
    "        return sum([a.h_index for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def variance_authors_h_index(self) -> float:\n",
    "        avg = self.avg_authors_h_index\n",
    "        return sum([(a.h_index - avg) ** 2 for a in self.feed.authors]) / len(self.feed.authors)\n",
    "    \n",
    "\n",
    "fe = FeatureExtractor([])\n",
    "fe.avg_authors_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billxbf/anaconda3/envs/arxplorer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 242.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 578.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ins = \"I like papers with innovative ideas instead of replication of existing methods on subfields. World modeling, planning and automation interest me most while others are also welcome.\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(ins)\n",
    "embedding2 = model.encode(\"Robotics\")\n",
    "embedding3 = model.encode(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marxplorer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marxplorer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding_L2_similarity\n\u001b[0;32m----> 4\u001b[0m embedding_L2_similarity(\u001b[43membeddings\u001b[49m, embedding3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from arxplorer.utils import *\n",
    "from arxplorer.utils import embedding_L2_similarity\n",
    "\n",
    "embedding_L2_similarity(embeddings, embedding3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not sure who Binfeng Xu is. Would you like me to look up more information for you?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arxplorer.utils import *\n",
    "\n",
    "ans = openai_chat_completion(\"who is binfeng xu?\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not sure who specifically you are referring to, but if you have any questions or need information about someone named Binfeng Xu, feel free to ask and I'll do my best to assist you.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Task Description\\nFollowing are 3 new papers with abstracts selected from arXiv cs.AI section.\\nConsider I\\'m familiar with this research field. Your task is to rank and recommend top 2 papers that match my preferences. \\nFor each of your recommendation, provide a 1~2 sentence summary followed by a 1~2 sentence explanation of your reasoning.\\n\\n## My Preferences\\nI like papers with innovative ideas instead of replication of existing methods on subfields. World modeling, planning and automation interest me most while others are also welcome.\\n\\n## Papers\\n\\n\\n## Template for Your Recommendations (use this json format)\\n{\\n    \"recommendations\": [\\n        {\\n            \"title\": \"Title of the paper\",\\n            \"summary\": \"1~2 sentence summary of the paper\",\\n            \"reasoning\": \"1~2 sentence explanation of your reasoning\"\\n        },\\n        {\\n            \"title\": \"Title of the paper\",\\n            \"summary\": \"1~2 sentence summary of the paper\",\\n            \"reasoning\": \"1~2 sentence explanation of your reasoning\"\\n        }\\n    ]\\n}\\n## Your Recommendations\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RERANK_PROMPT = '''\n",
    "## Task Description\n",
    "Following are {coarse_k} new papers with abstracts selected from arXiv {namespace} section.\n",
    "Consider I'm familiar with this research field. Your task is to rank and recommend top {top_k} papers that match my preferences. \n",
    "For each of your recommendation, provide a 1~2 sentence summary followed by a 1~2 sentence explanation of your reasoning.\n",
    "\n",
    "## My Preferences\n",
    "{instruction}\n",
    "\n",
    "## Papers\n",
    "{feeds}\n",
    "\n",
    "## Template for Your Recommendations (use this json format)\n",
    "{{\n",
    "    \"recommendations\": [\n",
    "        {{\n",
    "            \"title\": \"Title of the paper\",\n",
    "            \"summary\": \"1~2 sentence summary of the paper\",\n",
    "            \"reasoning\": \"1~2 sentence explanation of your reasoning\"\n",
    "        }},\n",
    "        {{\n",
    "            \"title\": \"Title of the paper\",\n",
    "            \"summary\": \"1~2 sentence summary of the paper\",\n",
    "            \"reasoning\": \"1~2 sentence explanation of your reasoning\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "## Your Recommendations\n",
    "'''\n",
    "\n",
    "RERANK_PROMPT.format(coarse_k=3, namespace=\"cs.AI\", top_k=2, instruction=\"I like papers with innovative ideas instead of replication of existing methods on subfields. World modeling, planning and automation interest me most while others are also welcome.\", feeds=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Exploring the Unseen: Novel Approaches in AI', 'summary': 'This paper introduces new methods for unsupervised learning in AI.', 'reasoning': 'The paper presents innovative techniques that align with my interest in world modeling.'}, {'title': 'Automated Planning for Robotics', 'summary': 'The study discusses advancements in automated planning algorithms for robotics.', 'reasoning': 'It relates to my preference for automation and planning in AI research.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "generated_text = '''\n",
    "{\n",
    "    \"recommendations\": [\n",
    "        {\n",
    "            \"title\": \"Exploring the Unseen: Novel Approaches in AI\",\n",
    "            \"summary\": \"This paper introduces new methods for unsupervised learning in AI.\",\n",
    "            \"reasoning\": \"The paper presents innovative techniques that align with my interest in world modeling.\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Automated Planning for Robotics\",\n",
    "            \"summary\": \"The study discusses advancements in automated planning algorithms for robotics.\",\n",
    "            \"reasoning\": \"It relates to my preference for automation and planning in AI research.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse the JSON string into a Python dictionary\n",
    "recommendations_dict = json.loads(generated_text)\n",
    "\n",
    "# Now you can access the data as a dictionary\n",
    "print(recommendations_dict['recommendations'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"recommendations\": [\n",
      "        {\n",
      "            \"title\": \"Exploring the Unseen: Novel Approaches in AI\",\n",
      "            \"summary\": \"This paper introduces new methods for unsupervised learning in AI.\",\n",
      "            \"reasoning\": \"The paper presents innovative techniques that align with my interest in world modeling.\"\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Automated Planning for Robotics\",\n",
      "            \"summary\": \"The study discusses advancements in automated planning algorithms for robotics.\",\n",
      "            \"reasoning\": \"It relates to my preference for automation and planning in AI research.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_string = json.dumps(recommendations_dict, indent=4)\n",
    "\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billxbf/anaconda3/envs/arxplorer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|ââââââââââ| 2/2 [00:03<00:00,  1.59s/it]\n",
      "generation_config.json: 100%|ââââââââââ| 124/124 [00:00<00:00, 836kB/s]\n",
      "tokenizer_config.json: 100%|ââââââââââ| 7.34k/7.34k [00:00<00:00, 13.2MB/s]\n",
      "vocab.json: 100%|ââââââââââ| 798k/798k [00:00<00:00, 3.33MB/s]\n",
      "merges.txt: 100%|ââââââââââ| 456k/456k [00:00<00:00, 3.83MB/s]\n",
      "tokenizer.json: 100%|ââââââââââ| 2.11M/2.11M [00:00<00:00, 8.63MB/s]\n",
      "added_tokens.json: 100%|ââââââââââ| 1.08k/1.08k [00:00<00:00, 13.7MB/s]\n",
      "special_tokens_map.json: 100%|ââââââââââ| 99.0/99.0 [00:00<00:00, 284kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "   ```\n",
      "\n",
      "2. Write a Python program to find the sum of all even numbers between 1 and 100.\n",
      "\n",
      "   Ideas: Use a for loop to iterate over all numbers between 1 and 100. Use an if statement to check if the number is even. If it is, add it to a running total.\n",
      "\n",
      "   ```python\n",
      "   total = 0\n",
      "   for i in range(1, 101):\n",
      "       if i % 2 == 0:\n",
      "           total += i\n",
      "   print(total)\n",
      "   ```\n",
      "\n",
      "3. Write a Python program to find the largest number in a list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "   ```\n",
      "\n",
      "2. Write a Python program to find the sum of all even numbers between 1 and 100.\n",
      "\n",
      "   Ideas: Use a for loop to iterate over all numbers between 1 and 100. Use an if statement to check if the number is even. If it is, add it to a running total.\n",
      "\n",
      "   ```python\n",
      "   total = 0\n",
      "   for i in range(1, 101):\n",
      "       if i % 2 == 0:\n",
      "           total += i\n",
      "   print(total)\n",
      "   ```\n",
      "\n",
      "3. Write a Python program to find the largest number in a list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/billxbf/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|ââââââââââ| 2/2 [00:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n):\n",
      "      if n % i == 0:\n",
      "         print(i)\n",
      "         break\n",
      "   else:\n",
      "      print(\"All primes between 1 and \" + str(n) + \" have been printed\")\n",
      "\n",
      "print_prime(100)\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from imp import lock_held\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Call this function to login to Hugging Face within the notebook environment\n",
    "login(\"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\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\n",
    "\n",
    "input_text = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:Prompt is set, the prompt will be automatically applied during the encoding phase. To disable prompt setting, please configure set_prompt(prompt=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37527996  0.08761922  0.52706105 ... -0.26766068  0.02662069\n",
      "  -0.14156012]]\n",
      "[[ 0.49359968  0.1666597   0.4639308  ... -0.162784   -0.10181545\n",
      "  -0.23928262]\n",
      " [ 0.2607212   0.6358187   0.88706136 ... -0.40785718 -0.25375775\n",
      "  -0.15815166]]\n"
     ]
    }
   ],
   "source": [
    "from angle_emb import AnglE, Prompts\n",
    "import torch\n",
    "\n",
    "# Assuming you want to use the CUDA device if available\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').to(device)\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "# Ensure the input is on the same device as the model\n",
    "vec = angle.encode({'text': 'hello world'}, to_numpy=True, device=device)\n",
    "print(vec)\n",
    "\n",
    "# Ensure the input is on the same device as the model\n",
    "vecs = angle.encode([{'text': 'hello world1'}, {'text': 'hello world2'}], to_numpy=True, device=device)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billxbf/anaconda3/envs/arxplorer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 papers in the cs.AI section of arXiv.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting data from papers ...: 100%|ââââââââââ| 125/125 [00:00<00:00, 11413.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from arxplorer.parsers import parse_arxiv, _parse_scholar\n",
    "from arxplorer.ranker import PaperRanker\n",
    "from arxplorer.datamodel import Config\n",
    "\n",
    "feeds = parse_arxiv(\"cs.AI\", fast_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"I like innovative papers in large foundation models, multimodal methods, symbolic planning and automation,  Others general ML topics are welcome, while direct applications in niche fields are less interesting.\"\n",
    "cfg = Config(top_k=10, coarse_k=20, instruction=instruction)\n",
    "ranker = PaperRanker(cfg)\n",
    "final_feeds = ranker.rank(feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resonance RoPE: Improving Context Length Generalization of Large  Language Models'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feeds[4].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Addressing the TSTL (Train-Short-Test-Long) problem, this paper introduces 'Resonance RoPE' to narrow the generalization gap in Large Language Models. By refining RoPE features for out-of-distribution positions, it enhances model performance in long-context scenarios.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feeds[4].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Addressing the TSTL (Train-Short-Test-Long) problem, this paper introduces 'Resonance RoPE' to narrow the generalization gap in Large Language Models. By refining RoPE features for out-of-distribution positions, it enhances model performance in long-context scenarios.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feeds[4].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width):\n",
    "    words = text.split(' ')\n",
    "    wrapped_lines = []\n",
    "    current_line = \"\"\n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) + 1 <= width:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            wrapped_lines.append(current_line.rstrip())\n",
    "            current_line = word + \" \"\n",
    "    wrapped_lines.append(current_line.rstrip())  # Add last line\n",
    "    return wrapped_lines\n",
    "\n",
    "def print_paper_metadata(papers):\n",
    "    max_width = 75  # Adjusted for consistent right border\n",
    "    for paper in papers:\n",
    "        print(\"+------------------------------------------------------------------------------+\")\n",
    "        title_lines = wrap_text(paper.title, max_width)\n",
    "        for line in title_lines:\n",
    "            print(f\"| {line.ljust(max_width)}  |\")\n",
    "        url_line = f\"| URL: {paper.pdf_url}\"\n",
    "        print(f\"{url_line.ljust(max_width + 2)}  |\")\n",
    "        print(\"+------------------------------------------------------------------------------+\")\n",
    "        if paper.summary:\n",
    "            print(\"| TL;DR: \".ljust(max_width + 3) + \" |\")\n",
    "            summary_lines = wrap_text(paper.summary, max_width)\n",
    "            for line in summary_lines:\n",
    "                print(f\"| {line.ljust(max_width)}  |\")\n",
    "            print(\"+------------------------------------------------------------------------------+\")\n",
    "        print()  # Print an empty line for spacing between papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "| Softened Symbol Grounding for Neuro-symbolic Systems                         |\n",
      "| URL: https://arxiv.org/pdf/2403.00323                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| This paper presents a new framework for neuro-symbolic learning that         |\n",
      "| bridges the gap between neural network training and symbolic constraint      |\n",
      "| solving, significantly enhancing the symbol grounding process. The           |\n",
      "| introduced approach demonstrates its efficacy with multiple neuro-symbolic   |\n",
      "| learning tasks.                                                              |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Learning with Logical Constraints but without Shortcut Satisfaction          |\n",
      "| URL: https://arxiv.org/pdf/2403.00329                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| This paper details a new neuro-symbolic learning framework that              |\n",
      "| circumvents the pitfall of âshortcut satisfactionâ by incorporating dual     |\n",
      "| variables for logical connectives in the learning process. The approach      |\n",
      "| enhances the model's generalizability and the ability to fulfill logical     |\n",
      "| constraints.                                                                 |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| SEED: Customize Large Language Models with Sample-Efficient Adaptation       |\n",
      "| for Code Generation                                                          |\n",
      "| URL: https://arxiv.org/pdf/2403.00046                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| This study proposes a novel adaptation approach, namely SEED, which          |\n",
      "| refines the performance of Large Language Models in code generation with     |\n",
      "| limited training data by exploiting errors and learning from them.           |\n",
      "| Demonstrated improvement showcases the efficiency of this method.            |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| UniTS: Building a Unified Time Series Model                                  |\n",
      "| URL: https://arxiv.org/pdf/2403.00131                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| The paper describes UniTS, an innovative framework adapted from foundation   |\n",
      "| models, designed for different tasks related to time series data such as     |\n",
      "| forecasting, classification, imputation, and anomaly detection. The model    |\n",
      "| has shown advantage over other traditional task-specific models in tests     |\n",
      "| across various multi-domain datasets.                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Resonance RoPE: Improving Context Length Generalization of Large  Language   |\n",
      "| Models                                                                       |\n",
      "| URL: https://arxiv.org/pdf/2403.00071                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| Addressing the TSTL (Train-Short-Test-Long) problem, this paper introduces   |\n",
      "| 'Resonance RoPE' to narrow the generalization gap in Large Language          |\n",
      "| Models. By refining RoPE features for out-of-distribution positions, it      |\n",
      "| enhances model performance in long-context scenarios.                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| On the Challenges and Opportunities in Generative AI                         |\n",
      "| URL: https://arxiv.org/pdf/2403.00025                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| This work discusses the challenges that large-scale generative AI models     |\n",
      "| face, which prevent their widespread utilization and adoption across         |\n",
      "| domains. By identifying unresolved issues, the authors aim to guide future   |\n",
      "| research into more robust and reliable generative AI solutions.              |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Axe the X in XAI: A Plea for Understandable AI                               |\n",
      "| URL: https://arxiv.org/pdf/2403.00315                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| The paper challenges the concept of explainability in AI, suggesting that    |\n",
      "| 'understandable AI' is a more suitable terminology. It argues for a          |\n",
      "| pragmatic interpretation of understanding an AI system by evaluating its     |\n",
      "| successful use rather than classical scientific explanations.                |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents       |\n",
      "| URL: https://arxiv.org/pdf/2403.00690                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| In an attempt to test the abilities of Large Language Models (LLMs) beyond   |\n",
      "| simple game-playing scenarios, this paper introduces NetPlay, an             |\n",
      "| LLM-powered agent for NetHack, a complex roguelike game. While NetPlay       |\n",
      "| exhibits proficiency with the game's mechanics, it does face challenges in   |\n",
      "| ambiguity and feedback.                                                      |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Deep Reinforcement Learning for Solving Management Problems: Towards A       |\n",
      "| Large Management Mode                                                        |\n",
      "| URL: https://arxiv.org/pdf/2403.00318                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| This paper introduces a new approach to apply deep reinforcement learning    |\n",
      "| in solving various management issues. The approach is aimed to unify the     |\n",
      "| decision-making patterns across different domains and demonstrated its       |\n",
      "| effectiveness in dynamic business environments.                              |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------+\n",
      "| Global and Local Prompts Cooperation via Optimal Transport for Federated     |\n",
      "| Learning                                                                     |\n",
      "| URL: https://arxiv.org/pdf/2403.00041                                        |\n",
      "+------------------------------------------------------------------------------+\n",
      "| TL;DR:                                                                       |\n",
      "| The paper presents FedOTP, a new strategy that integrates collaborative      |\n",
      "| prompt learning to address data heterogeneity in federated learning          |\n",
      "| frameworks. This technique uses both global and local prompts to balance     |\n",
      "| between global consensus and local personalization.                          |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_paper_metadata(final_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
